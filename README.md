# Weather Lakehouse (Databricks) â€” Bronze â†’ Silver â†’ Gold + Data Quality

Pipeline **Lakehouse** para transformar dados meteorolÃ³gicos em camadas **Silver/Gold** no **Databricks**, com **deduplicaÃ§Ã£o por execuÃ§Ã£o (`run_ts`)** e **relatÃ³rio de Data Quality**.

> âœ… **IngestÃ£o (Azure Functions) ficou em outro repositÃ³rio:**
> https://github.com/GabrielContesini/weather-lakehouse

---

## ğŸ¯ Objetivo

Demonstrar prÃ¡tica real de Engenharia de Dados com:

- arquitetura **Bronze/Silver/Gold**
- particionamento por data (`dt=YYYY-MM-DD`)
- dedupe por cidade pegando o `run_ts` mais recente
- outputs em **Parquet**
- checks automatizados de **Data Quality** + `dq_report.json`

---

## ğŸ§± Arquitetura

**Fonte** â†’ **Bronze (JSON)** â†’ **Silver (Parquet Hourly)** â†’ **Gold (Parquet Daily)** â†’ **DQ Report (JSON)**

- **Fonte**: Open-Meteo (Historical/Archive API)
- **Bronze**: JSON bruto (gerado pelo projeto de Azure Functions)
- **Databricks**:
  - leitura dos JSONs do Bronze
  - dedupe por cidade (`city_id`)
  - flatten da estrutura hourly â†’ **Silver**
  - agregaÃ§Ãµes diÃ¡rias â†’ **Gold**
  - validaÃ§Ãµes e relatÃ³rio â†’ **DQ**

---

## ğŸ“¦ Camadas e estrutura

### Bronze (entrada no Databricks)

VocÃª jÃ¡ tem os arquivos aqui:

/Volumes/weather/weather/raw/bronze/openmeteo/dt=YYYY-MM-DD/\*.json

### SaÃ­das (curated)

/Volumes/weather/weather/curated/silver/weather_hourly/dt=YYYY-MM-DD/.parquet
/Volumes/weather/weather/curated/gold/weather_daily/dt=YYYY-MM-DD/.parquet
/Volumes/weather/weather/curated/quality/reports/dt=YYYY-MM-DD/dq_report.json

---

## ğŸ““ Notebooks (portfÃ³lio)

SugestÃ£o de nomes bem â€œportfÃ³lioâ€ (ordem + aÃ§Ã£o + camada):

1. `01_bronze_ingestion_validation_and_dedupe`
   - LÃª os JSONs do Bronze
   - Deduplica: **1 arquivo por cidade** usando `run_ts` maior
   - (Opcional) valida presenÃ§a de colunas esperadas

2. `02_silver_hourly_weather_transform`
   - Flatten do `response.hourly` (arrays â†’ linhas)
   - Tipagem (double/timestamp)
   - Grava **Silver** em Parquet

3. `03_gold_daily_weather_aggregation`
   - Agrega por cidade/dia
   - Grava **Gold** em Parquet

4. `04_data_quality_report`
   - Checks de consistÃªncia
   - Gera `dq_report.json`

> VersÃ£o enxuta (2 notebooks):

- `01_bronze_to_silver_weather_lakehouse`
- `02_silver_to_gold_and_dq_weather_lakehouse`

---

## ğŸš€ Como rodar (Databricks)

### 1) Conferir se os JSONs estÃ£o no Volume

No notebook (Python):

```python
dt = "2026-02-25"
bronze_dir = f"dbfs:/Volumes/weather/weather/raw/bronze/openmeteo/dt={dt}/"
display(dbutils.fs.ls(bronze_dir))

Se listar os .json, segue.

2) Bronze â†’ (dedupe) â†’ Silver (hourly)

Deduplica por city_id usando o maior run_ts

Faz flatten do response.hourly

ObservaÃ§Ã£o importante (Unity Catalog):

NÃ£o use input_file_name() em volumes UC.

Se precisar do nome do arquivo, use _metadata.file_name/_metadata.file_path.

3) Silver â†’ Gold (daily)

Agrega por:

date, city_id, city_name, uf

MÃ©tricas:

avg_temp

max_wind

total_precip

avg_humidity

4) Data Quality

Checks aplicados:

null_time_utc == 0

null_city_id == 0

temperatura fora do range: < -20 ou > 55

umidade fora do range: < 0 ou > 100

precipitaÃ§Ã£o negativa

vento negativo

SaÃ­da:

/Volumes/weather/weather/curated/quality/reports/dt=YYYY-MM-DD/dq_report.json
âœ… Resultado esperado

Ao final, vocÃª terÃ¡:

Silver Parquet (hourly)

Gold Parquet (daily)

dq_report.json com ok: true/false

ğŸ” Boas prÃ¡ticas aplicadas

Particionamento por data (dt=YYYY-MM-DD)

DeduplicaÃ§Ã£o por cidade usando run_ts mais recente

Outputs em Parquet (Ã³timo para consumo analÃ­tico)

SeparaÃ§Ã£o clara por camadas (Bronze/Silver/Gold)

Data Quality automatizado com relatÃ³rio versionÃ¡vel

ğŸ”— RepositÃ³rios

IngestÃ£o / Azure Functions (HTTP â†’ Bronze no Azure Storage):
https://github.com/GabrielContesini/weather-lakehouse
```
